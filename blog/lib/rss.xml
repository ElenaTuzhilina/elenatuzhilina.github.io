<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Obsidian Vault]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>Obsidian Vault</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 29 Jul 2024 17:49:39 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 29 Jul 2024 17:49:38 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[Probabilistic view at multivariate analysis]]></title><description><![CDATA[ 
 <br><br>Factor analysis<br><br>Given -dimensional observation vector  the goal is to relate in to -dimensional latent vector , where . Factor analysis model assumes  and proposes the probabilistic model  This implies  As  is typically assumed to be diagonal, in factor analysis,  are conditionally independent given the values of laten variables . Thus,  are intended to explain the correlation between  and  represent the variability unique to a particular .<br>
<br><br>Probabilistic interpretation of CCA<br><br>Given two datasets with observations  and  and defining the sample covariance matrices by the canonical pairs are denoted by  and  where  and  are left and right singular vectors of  The canonical correlations are stored on the diagonal of .<br>
Consider model<br>
<img alt="PCCA.png" src="\images\pcca.png" style="width: 200px; max-width: 100%;"><br>
The MLE estimates of the parameters are:<br>
Here  are arbitrary matrices such that , and  and  are the corresponding truncated matrices.<br><a data-tooltip-position="top" aria-label="https://www.di.ens.fr/~fbach/probacca.pdf" rel="noopener" class="external-link" href="https://www.di.ens.fr/~fbach/probacca.pdf" target="_blank">A Probabilistic Interpretation of Canonical Correlation Analysis - Bach and Jordan - 2006</a> <br><br>Probabilistic interpretation of PCA<br><br>Assume the linear model for signal  and consider the probabilistic model  In other words, <img alt="PPCA.png" src="\images\ppca.png" style="width: 200px; max-width: 100%;"><br>Let  be the sample covariance matrix computed using the observations , and  is its SVD. The MLE of  is  for  it is  Here  and  are truncated versions of  and  and  is an arbitrary orthogonal rotation matrix.<br>
For  the MLE the variance parameter is  which has the interpretation of the average variance "lost" in the projection.<br>Proof
Denote . Take the derivative of then If  (the minimizer of the log-likelihood) then take SVD  Thus  If  then  is an eigenvector of  with  If  then  is arbitrary.
<br>One can show that Using Bayes's rule we obtain Denote  the conditional latent mean. Then is the optimal reconstruction of . If  then  and the projection of  simplifies to <a data-tooltip-position="top" aria-label="https://www.cs.columbia.edu/~blei/seminar/2020-representation/readings/TippingBishop1999.pdf" rel="noopener" class="external-link" href="https://www.cs.columbia.edu/~blei/seminar/2020-representation/readings/TippingBishop1999.pdf" target="_blank">Probabilistic Principal Component Analysis - Tipping and Bishop - 1999</a><br><br><br>Weyl's theorem<br><br>Let  be symmetric matrices. Let  denote the -th matrix eigenvalue. Then,<br>
A more general result is<br>
<br>This is related to perturbation of eigenvalues. Let  then eigenvalues of  estimate well the ones of  if  is small.<br><br>Principal angle<br><br>Consider two matrices  such that . Define the angle between subspaces  and   as<br>
General statement is defined through the SVD of . If<br>
then  are called principal angles.<br>Define  and  the projection matrices. We also define the distance between subspaces  and  as<br>
<br><br>Davis-Kahan theorem<br><br>Let  be symmetric matrices and define .<br>
Suppose  and are the eigen-decompositions. Where  represent the first  eigenvectors of  and  respectively.<br>
If there is a gap in eigenvalues, i.e.<br>
then the upper bound for the distance between the eigen-spaces  and  is<br>
Here  refers to the <a data-href="Principal angle" href="\perturbation-theory\principal-angle.html" class="internal-link" target="_self" rel="noopener">Principal angle</a>.<br>Proof
We note that<br>
thus<br>

<br> <a data-tooltip-position="top" aria-label="https://trungvietvu.github.io/notes/2020/DavisKahan" rel="noopener" class="external-link" href="https://trungvietvu.github.io/notes/2020/DavisKahan" target="_blank">Blog: Matrix Perturbation and Davis-Kahan Theorem</a><a data-tooltip-position="top" aria-label="http://yueqicao.top/2021/01/12/Davis-Kahan-s-Theorem/" rel="noopener" class="external-link" href="http://yueqicao.top/2021/01/12/Davis-Kahan-s-Theorem/" target="_blank">Blog: Davis-Kahan Theorem</a><br><br><br>Matrix products<br><br><br>Given  and  the Kronecker product is an  matrix defined as<br>
<br><br>Given  and  the Khatri-Rao product is an  matrix defined as<br>
It is also called the "matching column-wise" Kronecker product.<br><br>Given  and  the Hadamard product is an  matrix defined as<br>
<br>Properties
If  denotes the Moore-Penrose pseudoinverse of  then<br>

<br><a data-tooltip-position="top" aria-label="https://www.kolda.net/publication/TensorReview.pdf" rel="noopener" class="external-link" href="https://www.kolda.net/publication/TensorReview.pdf" target="_blank">Tensor Decompositions and Applications - Kolda and Bader - 2009</a><br><br>Tensor multiplication<br><br><br>Fibers are defined by fixing every index but one. For instance, given a third-order tensor , we have column (mode-), row (mode-) and tube (mode-) fibers ,  and .<br>Slices are defined by fixing all but two indices. For instance, given a third-order tensor , we have horizontal, lateral and frontal slices ,  and .<br>The mode- matricization of a tensor  is denoted by  and arranges mode- fibers to be the columns of the resulting matrix.<br><br>Given a tensor  and a matrix  the -mode product is denoted by  and is a tensor of size  with elements<br>
<br>Properties

<br><a data-tooltip-position="top" aria-label="https://www.kolda.net/publication/TensorReview.pdf" rel="noopener" class="external-link" href="https://www.kolda.net/publication/TensorReview.pdf" target="_blank">Tensor Decompositions and Applications - Kolda and Bader - 2009</a><br><br>CP decomposition<br><br>An -way tensor  has a rank-one tensor if<br>
where  represents the outer product.<br>
The CP decomposition of  decomposes it into a sum of rank-one tensors as<br>
Here  are the factor matrices with columns normalized to length one, and  is the scaling vector.<br><img alt="CP.png" src="\images\cp.png"><br>The mode- matricization is therefore<br>
<br>Note
CP stands for CANDECOMP (canonical decomposition)/PARAFAC (parallel factors).
<br><br>Uniqueness means that this is the only possible combination of rank-one tensors that sums to  with exception of the scaling<br>
and permutation<br>
A sufficient condition for CP uniqueness is<br>
Here  is the k-rank of a matrix  defined as the maximum value  such that any  columns are linearly independent.<br>A necessary condition for CP uniqueness<br>
<br><br>Let  be a third-order tensor. The approximation goal is thus<br>
By fixing  and  we can find  via least squares<br>
as . Then we normalize  as  and .<br>A more general algorithm for computing a CP decomposition for an th-order tensor  with  components is provided below.<br>
<img alt="CP-ALS.png" src="\images\cp-als.png"><br><a data-tooltip-position="top" aria-label="https://www.kolda.net/publication/TensorReview.pdf" rel="noopener" class="external-link" href="https://www.kolda.net/publication/TensorReview.pdf" target="_blank">Tensor Decompositions and Applications - Kolda and Bader - 2009</a><br><br>Tucker decomposition<br><br>It decomposes a tensor into a core tensor multiplied by a matrix along each mode. If  in an -way tensor then the Tucker model is<br>
Usually,  are assumed to be columnwise orthogonal and  is called the core tensor.<br><img alt="Tucker.png" src="\images\tucker.png" style="width: 600px; max-width: 100%;"><br>The mode- matricization is therefore<br>
<br><br>One way to compute decomposition is through higher-order SVD (HOSVD).<br>
<img alt="HOSVD.png" src="\images\hosvd.png" style="width: 500px; max-width: 100%;"><br>If  then the result is called truncated HOSVD. The truncated HOSVD is not optimal in terms of giving the best fit as measured by the norm of the difference.<br>Higher-order orthogonal iteration (HOOI) solves the following optimization problem<br>
<img alt="HOOI.png" src="\images\hooi.png"><br>Tucker decomposition is non-unique. Let , , are nonsingular matrices then<br>
<br><a data-tooltip-position="top" aria-label="https://www.kolda.net/publication/TensorReview.pdf" rel="noopener" class="external-link" href="https://www.kolda.net/publication/TensorReview.pdf" target="_blank">Tensor Decompositions and Applications - Kolda and Bader - 2009</a>]]></description><link>stats-blog.html</link><guid isPermaLink="false">Stats blog.md</guid><pubDate>Mon, 29 Jul 2024 17:49:31 GMT</pubDate><enclosure url="images\pcca.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="images\pcca.png"&gt;&lt;/figure&gt;</content:encoded></item></channel></rss>