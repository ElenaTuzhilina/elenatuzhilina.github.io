---
title: 'Clustering: hierarchical clustering and gaussian mixture models'
output:
  html_document:
    df_print: kable
  pdf_document: default
---

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE, fig.width = 4, fig.height = 4.5, warning = F, message = F)
theme_set(theme_bw())
```

### Helper functions

```{r, warning=FALSE}
library(ggplot2)
library(dplyr)
library(viridis)
library(ggfortify)
library(factoextra)
library(cluster)
library(plotly)
library(ggdendro)


scatterplot = function(X, M, cluster, label = F){
  if(length(unique(cluster)) == 1){
    plt = ggplot()+
      geom_point(X, mapping = aes(longtitude, latitude))+
      geom_point(M, mapping = aes(longtitude, latitude), shape = 4, size = 4)
    if(label) return(plt + geom_text(X, mapping = aes(longtitude, latitude, label = 1:nrow(X)), nudge_x = 0.1))
    else return(plt)
  }
  else{
    ggplot()+
      geom_point(data.frame(X, cluster = as.factor(cluster)), mapping = aes(longtitude, latitude, color = cluster))+
      geom_point(M, mapping = aes(longtitude, latitude), shape = 4, size = 4)+
      theme(legend.position = "none")
  } 
}

barplot = function(values){
  n = length(values)
  df = data.frame(value = values,  index = 1:n)
  ggplot(df, aes(index, value, fill = value)) + 
    geom_bar(color = "black", stat = "identity")+
    scale_fill_gradient2(low="#619CFF", mid="white", high="#F8766D")+
    scale_x_continuous(breaks = 1:n)+
    theme_bw()+
    theme(legend.position = "none")
}

heatmap = function(A){
  n = nrow(A)
  p = ncol(A)  
  df = data.frame(value = c(A),  i = 1:n, j = rep(1:p, rep(n, p)))
  ggplot(df, aes(j, i, fill = value)) + 
    geom_tile()+
    scale_fill_gradient(low="green", high = "red")+
    scale_y_reverse()+
    theme_void()
}

heatmap_bw = function(A){
  n = nrow(A)
  p = ncol(A)  
  df = data.frame(value = c(A),  i = 1:n, j = rep(1:p, rep(n, p)))
  ggplot(df, aes(j, i, fill = value)) + 
    geom_tile()+
    scale_fill_gradient(low="white", high = "black")+
    scale_y_reverse()+
    theme_void()
}


gmmhist = function(X, Pi, M, S, title = NULL){
  col = c("forestgreen", "orange", "steelblue", "purple", "yellow", "salmon")
  K = length(Pi)
  plt = ggplot() +  geom_histogram(X, mapping = aes(x1, y = ..density..), alpha = 0.5) + theme(legend.position = "none")
  xs = seq(min(X[,1]), max(X[,1]), length.out = 100)
  for(k in 1:K){
    mu = M[k]
    pro = Pi[k]
    center = data.frame(x1 = mu)
    if(length(S) > 1) Sigma = S[k]
    else Sigma = S
    plt = plt + 
      geom_point(data = center, mapping = aes(x1, 0), color = col[k], shape = 4, size = 4)+
      geom_line(data = data.frame(x1 = xs, density = pro * dnorm(xs, mu,  sqrt(Sigma))), mapping = aes(x1, density), color = col[k], size = 1)
  }
  plt + ggtitle(title)
}

gmmplot = function(X, Z, Pi, M, S){
  col = c("forestgreen", "orange", "steelblue", "purple", "yellow", "salmon")
  K = ncol(Z)
  c = qchisq(0.8, 2)
  ts = seq(0, 2 * pi, 0.01)
  circle = data.frame(longtitude = sqrt(c) * cos(ts), latitude = sqrt(c) * sin(ts))
  plt = ggplot() + theme(legend.position = "none") + coord_equal()
  for(k in 1:K){
    df = data.frame(X, alpha = Z[,k])
    mu = M[,k]
    pro = Pi[k]
    center = data.frame(longtitude = mu[1], latitude = mu[2])
    Sigma = S[,,k]
    ED = eigen(Sigma)
    ellipse = t(ED$vectors %*% diag(sqrt(ED$values)) %*% t(circle) + mu)
    colnames(ellipse) = c("longtitude", "latitude")
    plt = plt + geom_point(df, mapping = aes(longtitude, latitude, alpha = alpha), color = col[k]) +
      geom_point(data = center, mapping = aes(longtitude, latitude), color = col[k], shape = 4, size = 4)+
      geom_path(data = ellipse, mapping = aes(longtitude, latitude), color = col[k])
  }
  plt
}
```


### Hierarchical clustering

Load microarray data.
```{r, fig.width = 7}
X = read.csv("data/cancer_microarray_subsample.csv", row.names = 1) %>% as.matrix()
dim(X)
X = scale(X, scale = T, center = T)
heatmap(X)
```


Let's check the PCA plot. It seems there is one outlier.
```{r}
PCA =  prcomp(X)
autoplot(PCA)+
  theme(legend.position = "none")
```


Compute pairwise distances first.
```{r, fig.height=6, fig.width=6}
D = dist(X, method = 'euclidean')
heatmap_bw(as.matrix(D))
```


Try different hierarchical clustering approaches.
```{r, fig.width=10, fig.height=10}
HCc = hclust(D, method = 'complete')
plot(HCc)
HCs = hclust(D, method = 'single')
plot(HCs)
HCa = hclust(D, method = 'average')
plot(HCa)
```

We will use complete linkage. We explore the output of the `hclust()` function first.
```{r, fig.height=10, fig.width=10}
plt = ggdendrogram(HCc)
plt
HCc$height
plt+
  geom_hline(aes(yintercept = HCc$height), color = "red", linetype = "dashed")
```


Permute genes according to the hierarchical clustering order.
```{r, fig.width = 7}
heatmap(X[HCc$order,])
```


What will happen to the pairwise distances heatmap?
```{r, fig.height=6, fig.width=6}
heatmap_bw(as.matrix(D)[HCc$order,HCc$order])
```

Let's cut the dendrogram at $K=2,\ldots,6$ clusters anc check the PCA result.
```{r}
col = c("#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#CC79A7", "black")
for(k in 2:6){
  CT = cutree(HCc, k = k)
  print(autoplot(PCA, data = data.frame(X, cluster = as.factor(CT)), shape = F, color = "cluster")+
    theme(legend.position = "none")+
      scale_color_manual(values = col))
}
```


More fancy way to do this.
```{r, fig.height=10, fig.width=10}
library(factoextra)
fviz_dend(HCc, k = 6,  color_labels_by_k = TRUE, rect = TRUE)
fviz_cluster(list(data = X, cluster = CT))+
  theme(legend.position = "none")
```

### Gaussian Mixture Model

Let's apply GMM to the data in $p=2$.
```{r}
set.seed(6)
n = 300
K = 3
M = matrix(rnorm(K), K, 1)
S = matrix(c(1, 0.7, 0.5), 3, 1)
colnames(M) =  c("x1")
rat = c(1/6, 1/3, 1/2)
Ms = M[rep(c(1, 2, 3), times = n*rat),]
Ss = S[rep(c(1, 2, 3), times = n*rat),]
X = matrix(rnorm(n, 0, 0.3), n, 1) * Ss + Ms
colnames(X) = c("x1")
plt = ggplot()+
  geom_histogram(X, mapping = aes(x1, y = ..density..), alpha = 0.5)
plt
```

Fit the GMM model.
```{r}
library(mclust)
GMM = Mclust(X, G = 3)
```


Model "V" means that the distribution is variable variance.
```{r}
GMM$modelName
```

Obtain the parameters of the fit.
```{r}
GMM$parameters$pro
GMM$parameters$mean
GMM$parameters$variance$sigmasq
head(GMM$z)
gmmhist(X, Pi = GMM$parameters$pro, M = GMM$parameters$mean, S = GMM$parameters$variance$sigmasq)
```


Use `modelNames` to control the type of the variance used for fitting.
Model "E" means that the distribution is equal variance.
```{r}
GMM = Mclust(X, G = 3, modelNames = "E")
gmmhist(X, Pi = GMM$parameters$pro, M = GMM$parameters$mean, S = GMM$parameters$variance$sigmasq)
```

Vary `G` to control the number of clusters.
```{r}
for(G in 2:6){
  GMM = Mclust(X, G = G, modelNames = "V", verbose = F)
  print(gmmhist(X, Pi = GMM$parameters$pro, M = GMM$parameters$mean, S = GMM$parameters$variance$sigmasq))
}
```



Let's see the convergence process.
```{r}
G = 3
for(iter in seq(1, 25, 8)){
  GMM = Mclust(X, G = G, modelNames = "V", control = emControl(itmax = iter),  verbose = F)
  print(gmmhist(X, Pi = GMM$parameters$pro, M = GMM$parameters$mean, S = GMM$parameters$variance$sigmasq, title = paste("iteration = ", iter)))
}
```


Now consider $p=3$.
```{r}
set.seed(6)
n = 180
K = 3
M = matrix(rnorm(K * 2), K, 2)
S = matrix(c(1, 0.7, 0.5, 1, 0.7, 0.5), 3, 2)
colnames(M) =  c("longtitude", "latitude")
rat = c(1/6, 1/3, 1/2)
Ms = M[rep(c(1, 2, 3), times = n*rat),]
Ss = S[rep(c(1, 2, 3), times = n*rat),]
X = matrix(rnorm(n * 2, 0, 0.5), n, 2) * Ss + Ms
colnames(X) = c("longtitude", "latitude")
plt = ggplot()+
  geom_point(X, mapping = aes(longtitude, latitude))
plt
```


Fit the GMM model.
```{r}
GMM = Mclust(X, G = 3, control = emControl(itmax = 100),  verbose = F)
```


Model "VII" means that the distribution is spherical with unequal volume.
```{r}
GMM$modelName
```

Obtain the parameters of the fit.
```{r}
GMM$parameters$pro
GMM$parameters$mean
GMM$parameters$variance$sigma
head(GMM$z)
gmmplot(X, Z = GMM$z, Pi = GMM$parameters$pro, M = GMM$parameters$mean, S = GMM$parameters$variance$sigma)
```


Use `modelNames` to control the type of the variance used for fitting.
"EII" is spherical, equal volume
```{r}
GMM = Mclust(X, G = 3, modelNames = "EII")
gmmplot(X, Z = GMM$z, Pi = GMM$parameters$pro, M = GMM$parameters$mean, S = GMM$parameters$variance$sigma)
```

"EEE" is ellipsoidal, equal volume, shape, and orientation.
```{r}
GMM = Mclust(X, G = 3, modelNames = "EEE")
gmmplot(X, Z = GMM$z, Pi = GMM$parameters$pro, M = GMM$parameters$mean, S = GMM$parameters$variance$sigma)
```

"VVV" is ellipsoidal, varying volume, shape, and orientation.
```{r}
GMM = Mclust(X, G = 3, modelNames = "VVV")
gmmplot(X, Z = GMM$z, Pi = GMM$parameters$pro, M = GMM$parameters$mean, S = GMM$parameters$variance$sigma)
```

Vary `G` to control the number of clusters.
```{r}
for(G in 2:6){
  GMM = Mclust(X, G = G, modelNames = "VVV", verbose = F)
  print(gmmplot(X, Z = GMM$z, Pi = GMM$parameters$pro, M = GMM$parameters$mean, S = GMM$parameters$variance$sigma))
}
```




